[info[msg]org.apache.hadoop.util.LogAdapter.info(LogAdapter.java:51), info[msg]org.apache.hadoop.util.LogAdapter.info(LogAdapter.java:51), info[$stack14]org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode(NameNode.java:1707), info["Loaded properties from {}",fname]org.apache.hadoop.metrics2.impl.MetricsConfig.loadFirst(MetricsConfig.java:120), info[$stack20]org.apache.hadoop.metrics2.impl.MetricsSystemImpl.startTimer(MetricsSystemImpl.java:378), info[$stack19]org.apache.hadoop.metrics2.impl.MetricsSystemImpl.start(MetricsSystemImpl.java:191), info["{} is {}","fs.defaultFS",nnAddr]org.apache.hadoop.hdfs.server.namenode.NameNodeUtils.getClientNamenodeAddress(NameNodeUtils.java:79), info["Clients should use {} to access this namenode/service.",$stack33]org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:1009), info[$stack27]org.apache.hadoop.hdfs.DFSUtil.httpServerTemplateForNNAndJN(DFSUtil.java:1668), info[$stack83]org.apache.hadoop.hdfs.DFSUtil.httpServerTemplateForNNAndJN(DFSUtil.java:1690), info["Unable to initialize FileSignerSecretProvider, falling back to use random secrets."]org.apache.hadoop.security.authentication.server.AuthenticationFilter.constructSecretProvider(AuthenticationFilter.java:240), info["Http request log for {} is not defined",loggerName]org.apache.hadoop.http.HttpRequestLog.getRequestLog(HttpRequestLog.java:82), info[$stack24]org.apache.hadoop.http.HttpServer2.addGlobalFilter(HttpServer2.java:1111), info[$stack13]org.apache.hadoop.http.HttpServer2.addJerseyResourcePackage(HttpServer2.java:938), info[$stack6]org.apache.hadoop.http.HttpServer2.bindListener(HttpServer2.java:1332), warn["Only one image storage directory (dfs.namenode.name.dir) configured. Beware of data loss due to lack of redundant storage directories!"]org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkConfiguration(FSNamesystem.java:745), warn["Only one namespace edits storage directory (dfs.namenode.edits.dir) configured. Beware of data loss due to lack of redundant storage directories!"]org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkConfiguration(FSNamesystem.java:750), info[$stack9]org.apache.hadoop.hdfs.server.namenode.FSEditLog.newInstance(FSEditLog.java:229), info[$stack20]org.apache.hadoop.hdfs.server.namenode.FSNamesystem.<init>(FSNamesystem.java:814), info[$stack18]org.apache.hadoop.hdfs.server.namenode.FSNamesystemLock.<init>(FSNamesystemLock.java:141), info[$stack32]org.apache.hadoop.hdfs.server.namenode.FSNamesystemLock.<init>(FSNamesystemLock.java:159), info[$stack39]org.apache.hadoop.hdfs.server.namenode.FSNamesystem.<init>(FSNamesystem.java:847), info[$stack45]org.apache.hadoop.hdfs.server.namenode.FSNamesystem.<init>(FSNamesystem.java:848), info[$stack51]org.apache.hadoop.hdfs.server.namenode.FSNamesystem.<init>(FSNamesystem.java:849), info[$stack57]org.apache.hadoop.hdfs.server.namenode.FSNamesystem.<init>(FSNamesystem.java:850), info[$stack67]org.apache.hadoop.hdfs.server.namenode.FSNamesystem.<init>(FSNamesystem.java:861), info[$stack13]org.apache.hadoop.hdfs.server.common.Util.isDiskStatsEnabled(Util.java:395), info[$stack73]org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager.<init>(DatanodeManager.java:334), info[$stack80]org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager.<init>(DatanodeManager.java:342), info["{} is set to {}","dfs.namenode.startup.delay.block.deletion.sec",$stack7]org.apache.hadoop.hdfs.server.blockmanagement.InvalidateBlocks.printBlockDeletionTime(InvalidateBlocks.java:77), info["The block deletion will start around {}",$stack16]org.apache.hadoop.hdfs.server.blockmanagement.InvalidateBlocks.printBlockDeletionTime(InvalidateBlocks.java:83), info[$stack37#23]org.apache.hadoop.util.LightWeightGSet.computeCapacity(LightWeightGSet.java:395), info[$stack43#42]org.apache.hadoop.util.LightWeightGSet.computeCapacity(LightWeightGSet.java:396), info[$stack54#83]org.apache.hadoop.util.LightWeightGSet.computeCapacity(LightWeightGSet.java:397), info[$stack62#110]org.apache.hadoop.util.LightWeightGSet.computeCapacity(LightWeightGSet.java:402), info["Storage policy satisfier is disabled"]org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.createSPSManager(BlockManager.java:5447), info["{} = {}","dfs.block.access.token.enable",$stack20]org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.createBlockTokenSecretManager(BlockManager.java:641), info["{} = {}","dfs.namenode.safemode.threshold-pct",$stack29]org.apache.hadoop.hdfs.server.blockmanagement.BlockManagerSafeMode.<init>(BlockManagerSafeMode.java:160), info["{} = {}","dfs.namenode.safemode.min.datanodes",$stack34]org.apache.hadoop.hdfs.server.blockmanagement.BlockManagerSafeMode.<init>(BlockManagerSafeMode.java:161), info["{} = {}","dfs.namenode.safemode.extension",$stack39]org.apache.hadoop.hdfs.server.blockmanagement.BlockManagerSafeMode.<init>(BlockManagerSafeMode.java:163), info["defaultReplication         = {}",$stack95]org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.<init>(BlockManager.java:627), info["maxReplication             = {}",$stack99]org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.<init>(BlockManager.java:628), info["minReplication             = {}",$stack103]org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.<init>(BlockManager.java:629), info["maxReplicationStreams      = {}",$stack107]org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.<init>(BlockManager.java:630), info["redundancyRecheckInterval  = {}ms",$stack111]org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.<init>(BlockManager.java:631), info["encryptDataTransfer        = {}",$stack115]org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.<init>(BlockManager.java:632), info["maxNumBlocksToLog          = {}",$stack119]org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.<init>(BlockManager.java:633), info["Lock on {} acquired by nodename {}",lockF,jvmName]org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory.tryLock(Storage.java:948), info[$stack11]org.apache.hadoop.hdfs.server.namenode.FileJournalManager.recoverUnfinalizedSegments(FileJournalManager.java:428), info["No edit log streams selected."]org.apache.hadoop.hdfs.server.namenode.FSImage.loadFSImage(FSImage.java:734), info[$stack11]org.apache.hadoop.hdfs.server.namenode.FSImage.loadFSImageFile(FSImage.java:800), info[$stack17]org.apache.hadoop.hdfs.server.namenode.FSImageFormatPBINode$Loader.loadINodeSectionHeader(FSImageFormatPBINode.java:413), info["Successfully loaded {} inodes",$stack12]org.apache.hadoop.hdfs.server.namenode.FSImageFormatPBINode$Loader.loadINodeSection(FSImageFormatPBINode.java:371), info["Completed update blocks map and name cache, total waiting duration {}ms.",$stack10]org.apache.hadoop.hdfs.server.namenode.FSImageFormatPBINode$Loader.waitBlocksMapAndNameCacheUpdateFinished(FSImageFormatPBINode.java:344), info["Loaded FSImage in {} seconds.",$stack20]org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf$Loader.load(FSImageFormatProtobuf.java:255), info[$stack21]org.apache.hadoop.hdfs.server.namenode.FSImage.loadFSImage(FSImage.java:978), info[$stack28#75]org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFSImage(FSNamesystem.java:1202), info[$stack13]org.apache.hadoop.hdfs.server.namenode.FSEditLog.startLogSegment(FSEditLog.java:1391), info[$stack10]org.apache.hadoop.hdfs.server.namenode.NameCache.initialized(NameCache.java:143), info[$stack22]org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFromDisk(FSNamesystem.java:786), info[$stack82]org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.<init>(NameNodeRpcServer.java:454), info[$stack88]org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.<init>(NameNodeRpcServer.java:459), info["Registered FSNamesystemState, ReplicatedBlocksState and ECBlockGroupsState MBeans."]org.apache.hadoop.hdfs.server.namenode.FSNamesystem.registerMBean(FSNamesystem.java:5444), info["Number of blocks under construction: {}",$stack18]org.apache.hadoop.hdfs.server.namenode.LeaseManager.getNumUnderConstructionBlocks(LeaseManager.java:166), info["initializing replication queues"]org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.initializeReplQueues(BlockManager.java:5187), info["STATE* Leaving safe mode after {} secs",$stack21]org.apache.hadoop.hdfs.server.blockmanagement.BlockManagerSafeMode.leaveSafeMode(BlockManagerSafeMode.java:399), info["STATE* Network topology has {} racks and {} datanodes",$stack29,$stack31]org.apache.hadoop.hdfs.server.blockmanagement.BlockManagerSafeMode.leaveSafeMode(BlockManagerSafeMode.java:405), info["STATE* UnderReplicatedBlocks has {} blocks",$stack36]org.apache.hadoop.hdfs.server.blockmanagement.BlockManagerSafeMode.leaveSafeMode(BlockManagerSafeMode.java:407), info[$stack21]org.apache.hadoop.hdfs.server.namenode.NameNode.startCommonServices(NameNode.java:893), info["Starting services required for active state"]org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startActiveServices(FSNamesystem.java:1314), info[$stack14]org.apache.hadoop.hdfs.server.namenode.FSDirectory.updateCountForQuota(FSDirectory.java:840), info[$stack33]org.apache.hadoop.hdfs.server.namenode.FSDirectory.updateCountForQuota(FSDirectory.java:849), info[$stack22]org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor.run(CacheReplicationMonitor.java:160), info[$stack15]org.apache.hadoop.hdfs.server.namenode.FSEditLog.endCurrentLogSegment(FSEditLog.java:1441), info[$stack29]org.apache.hadoop.hdfs.server.namenode.FSEditLog.printStatistics(FSEditLog.java:778), info[$stack29]org.apache.hadoop.hdfs.server.namenode.FSEditLog.printStatistics(FSEditLog.java:778), info[$stack18]org.apache.hadoop.hdfs.server.namenode.FileJournalManager.finalizeLogSegment(FileJournalManager.java:145)]
[info[$stack43]org.apache.hadoop.hdfs.server.namenode.SerialNumberManager.<clinit>(SerialNumberManager.java:51), info[$stack43]org.apache.hadoop.hdfs.server.namenode.SerialNumberManager.<clinit>(SerialNumberManager.java:51), info[$stack43]org.apache.hadoop.hdfs.server.namenode.SerialNumberManager.<clinit>(SerialNumberManager.java:51), info[$stack43]org.apache.hadoop.hdfs.server.namenode.SerialNumberManager.<clinit>(SerialNumberManager.java:51)]
[info[msg]org.apache.hadoop.util.LogAdapter.info(LogAdapter.java:51)]